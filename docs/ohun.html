<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>ohun.knit</title>

<script src="site_libs/header-attrs-2.13/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">OTS Bioacoustic Analysis in R 2022</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="program.html">Program</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="introduction.html">Introduction</a>
    </li>
    <li>
      <a href="r_basics.html">R basics</a>
    </li>
    <li>
      <a href="sound.html">Sound</a>
    </li>
    <li>
      <a href="spectrograms.html">Spectrograms</a>
    </li>
    <li>
      <a href="seewave.html">seewave</a>
    </li>
    <li>
      <a href="annotations.html">Annotations</a>
    </li>
    <li>
      <a href="ohun.html">ohun: automatic detection</a>
    </li>
    <li>
      <a href="intro_to_warbler.html">Intro to warbleR</a>
    </li>
    <li>
      <a href="measure_acoustic_structure.html">Measure acoustic structure</a>
    </li>
    <li>
      <a href="comparing_methods.html">Comparing measuring methods</a>
    </li>
    <li>
      <a href="quality_checks.html">Quality checks</a>
    </li>
    <li>
      <a href="measure_structure_green_hermit.html">Measure structure on XC recordings</a>
    </li>
  </ul>
</li>
<li>
  <a href="course_prep.html">Course prep</a>
</li>
<li>
  <a href="instructor.html">Instructor</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore"><center>
<font size="7"><b><i>ohun</i>: optimizing acoustic signal detection</b></font>
</center></h1>
<h3 class="subtitle"><center>
<font size="4"><b>Bioacoustic Analysis in R</b> <br> Organization for Tropical Studies</font>
</center></h3>
<h4 class="author"><center>
<font size="3"><a href="http://marceloarayasalas.weebly.com/">Marcelo Araya-Salas, PhD</a></font>
</center></h4>
<h4 class="date"><center>
“2022-05-14”
</center></h4>

</div>


<p><a href="https://github.com/maRce10/ohun">ohun</a> is intended to facilitate the automatic detection of acoustic signals, providing functions to diagnose and optimize detection routines. Detections from other software can also be explored and optimized.</p>
<p> </p>
<div class="alert alert-info">
<p><font size = "4">The main features of the package are: </font></p>
<ul>
<li>The use of reference annotations for detection optimization and diagnostic</li>
<li>The use of signal detection theory indices to evaluate detection performance</li>
<li>Batch processing of sound files for improving computational performance</li>
</ul>
<p> </p>
<p><font size = "4">The package offers functions for: </font></p>
<ul>
<li>Diagnose detection performance</li>
<li>Optimize detection routines based on reference annotations</li>
<li>Energy-based detection</li>
<li>Template-based detection</li>
</ul>
</div>
<p>All functions allow the parallelization of tasks, which distributes the tasks among several processors to improve computational efficiency. The package works on sound files in ‘.wav’, ‘.mp3’, ‘.flac’ and ‘.wac’ format.</p>
<hr />
<p>To install the latest developmental version from <a href="https://github.com/">github</a> you will need the R package <a href="https://cran.r-project.org/package=devtools">devtools</a>:</p>
<pre class="r"><code># install pacakge
devtools::install_github(&quot;maRce10/ohun&quot;)

#load package
library(ohun)</code></pre>
<p> </p>
<hr />
<div id="automatic-signal-detection" class="section level1">
<h1>Automatic signal detection</h1>
<p>Finding the position of signals in a sound file is a challenging task. <a href="https://github.com/maRce10/ohun">ohun</a> offers two methods for automatic signal detection: template-based and energy-based detection. These methods are better suited for highly stereotyped or good signal-to-noise ratio (SNR) signals, respectively. If the target signals don’t fit these requirements, more elaborated methods (i.e. machine learning approaches) are warranted:</p>
<figure>
<center>
<img src="analysis_workflow.png" alt="automatic signal detection diagram" width="500" height="450">
</center>
<figcaption>
<i>Diagram depicting how target signal features can be used to tell the most adequate acoustic signal detection approach. Steps in which ‘ohun’ can be helpful are shown in color. (SNR = signal-to-noise ratio) </i>
</figcaption>
</figure>
<p> </p>
<p>Still, a detection run using other software can be optimized with the tools provided in <a href="https://github.com/maRce10/ohun">ohun</a>.</p>
<p> </p>
</div>
<div id="signal-detection-theory-applied-to-bioacoustics" class="section level1">
<h1>Signal detection theory applied to bioacoustics</h1>
<p>Broadly speaking, signal detection theory deals with the process of recovering signals (i.e. target signals) from background noise (not necessarily acoustic noise) and it’s widely used for optimizing this decision making process in the presence of uncertainty. During a detection routine, the detected ‘items’ can be classified into 4 classes:</p>
<ul>
<li><strong>True positives (TPs)</strong>: signals correctly identified as ‘signal’</li>
<li><strong>False positives (FPs)</strong>: background noise incorrectly identified as ‘signal’</li>
<li><strong>False negatives (FNs)</strong>: signals incorrectly identified as ‘background noise’</li>
<li><strong>True negatives (TNs)</strong>: background noise correctly identified as ‘background noise’</li>
</ul>
<p>Several additional indices derived from these indices are used to evaluate the performance of a detection routine. These are three useful indices in the context of acoustic signal detection included in <a href="https://github.com/maRce10/ohun">ohun</a>:</p>
<ul>
<li><strong>Recall</strong>: correct detections relative to total detections (a.k.a. true positive rate or sensitivity; <em>TPs / (TPs + FNs)</em>)</li>
<li><strong>Precision</strong>: correct detections relative to total detections (<em>TPs / (TPs + FPs)</em>).</li>
<li><strong>F1 score</strong>: combines recall and precision as the harmonic mean of these two, so it provides a single value for evaluating performance (a.k.a. F-measure or Dice similarity coefficient).</li>
</ul>
<p><font size = "2"><em>(Metrics that make use of ‘true negatives’ cannot be easily applied in the context of acoustic signal detection as noise cannot always be partitioned in discrete units)</em></font></p>
<p>A perfect detection will have no false positives or false negatives, which will result in both recall and precision equal to 1. However, perfect detection cannot always be reached and some compromise between detecting all target signals plus some noise (recall = 1 &amp; precision &lt; 1) and detecting only target signals but not all of them (recall &lt; 1 &amp; precision = 1) is warranted. The right balance between these two extremes will be given by the relative costs of missing signals and mistaking noise for signals. Hence, these indices provide an useful framework for diagnosing and optimizing the performance of a detection routine.</p>
<p>The package <a href="https://github.com/maRce10/ohun">ohun</a> provides a set of tools to evaluate the performance of an acoustic signal detection based on the indices described above. To accomplish this, the result of a detection routine is compared against a reference table containing the time position of all target signals in the sound files. The package comes with an example reference table containing annotations of long-billed hermit hummingbird songs from two sound files (also supplied as example data: ‘lbh1’ and ‘lbh2’), which can be used to illustrate detection performance evaluation. The example data can be explored as follows:</p>
<pre class="r"><code># load example data
data(&quot;lbh1&quot;, &quot;lbh2&quot;, &quot;lbh_reference&quot;)

lbh_reference</code></pre>
<pre><code>## Object of class &#39;selection_table&#39; 
## * The output of the following call: 
## warbleR::selection_table(X = lbh_reference) 
## 
## Contains: 
## *  A selection table data frame with 19 rows and 6 columns: 
## |sound.files | selec|  start|    end| bottom.freq| top.freq|
## |:-----------|-----:|------:|------:|-----------:|--------:|
## |lbh2.wav    |     1| 0.1092| 0.2482|      2.2954|   8.9382|
## |lbh2.wav    |     2| 0.6549| 0.7887|      2.2954|   9.0426|
## |lbh2.wav    |     3| 1.2658| 1.3856|      2.2606|   9.0774|
## |lbh2.wav    |     4| 1.8697| 2.0053|      2.1911|   8.9035|
## |lbh2.wav    |     5| 2.4418| 2.5809|      2.1563|   8.6600|
## |lbh2.wav    |     6| 3.0368| 3.1689|      2.2259|   8.9382|
## ... and 13 more row(s) 
## 
##  * A data frame (check.results) generated by check_sels() (as attribute) 
## created by warbleR 1.1.27</code></pre>
<p> </p>
<p>This is a ‘selection table’, an object class provided by the package <a href="https://CRAN.R-project.org/package=warbleR">warbleR</a> (see <a href="https://marce10.github.io/warbleR/reference/selection_table.html"><code>selection_table()</code></a> for details). Selection tables are basically data frames in which the contained information has been double-checked (using warbleR’s <a href="https://marce10.github.io/warbleR/reference/check_sels.html"><code>check_sels()</code></a>). But they behave pretty much as data frames and can be easily converted to data frames:</p>
<pre class="r"><code># convert to data frame
as.data.frame(lbh_reference)</code></pre>
<pre><code>##    sound.files selec    start       end bottom.freq top.freq
## 1     lbh2.wav     1 0.109161 0.2482449      2.2954   8.9382
## 2     lbh2.wav     2 0.654921 0.7887232      2.2954   9.0426
## 3     lbh2.wav     3 1.265850 1.3855678      2.2606   9.0774
## 4     lbh2.wav     4 1.869705 2.0052678      2.1911   8.9035
## 5     lbh2.wav     5 2.441769 2.5808529      2.1563   8.6600
## 6     lbh2.wav     6 3.036825 3.1688667      2.2259   8.9382
## 7     lbh2.wav     7 3.628617 3.7465742      2.3302   8.6252
## 8     lbh2.wav     8 4.153288 4.2818085      2.2954   8.4861
## 9     lbh2.wav     9 4.723673 4.8609963      2.3650   8.6948
## 10    lbh1.wav    10 0.088118 0.2360047      1.9824   8.4861
## 11    lbh1.wav    11 0.572290 0.7201767      2.0520   9.5295
## 12    lbh1.wav    12 1.056417 1.1972614      2.0868   8.4861
## 13    lbh1.wav    13 1.711338 1.8680274      1.9824   8.5905
## 14    lbh1.wav    14 2.190249 2.3416568      2.0520   8.5209
## 15    lbh1.wav    15 2.697143 2.8538324      1.9824   9.2513
## 16    lbh1.wav    16 3.181315 3.3344833      1.9129   8.4861
## 17    lbh1.wav    17 3.663719 3.8133662      1.8781   8.6948
## 18    lbh1.wav    18 4.140816 4.3045477      1.8433   9.2165
## 19    lbh1.wav    19 4.626712 4.7851620      1.8085   8.9035</code></pre>
<p> </p>
<p>All <a href="https://github.com/maRce10/ohun">ohun</a> functions that work with this kind of data can take both selection tables and data frames. Spectrograms with highlighted signals from a selection table can be plotted with the function <code>label_spectro()</code> (this function only plots one wave object at the time):</p>
<pre class="r"><code># save sound file
writeWave(lbh1, file.path(tempdir(), &quot;lbh1.wav&quot;))

# save sound file
writeWave(lbh2, file.path(tempdir(), &quot;lbh2.wav&quot;))

# print spectrogram
label_spectro(wave = lbh1, reference = lbh_reference[lbh_reference$sound.files ==
    &quot;lbh1.wav&quot;, ], hop.size = 10, ovlp = 50, flim = c(1, 10))</code></pre>
<p><img src="ohun_files/figure-html/unnamed-chunk-4-1.png" width="960" /></p>
<pre class="r"><code># print spectrogram
label_spectro(wave = lbh2, reference = lbh_reference[lbh_reference$sound.files ==
    &quot;lbh2.wav&quot;, ], hop.size = 10, ovlp = 50, flim = c(1, 10))</code></pre>
<p><img src="ohun_files/figure-html/unnamed-chunk-4-2.png" width="960" /></p>
<p>The function <code>diagnose_detection()</code> evaluates the performance of a detection routine by comparing it to a reference table. For instance, a perfect detection is given by comparing <code>lbh_reference</code> to itself:</p>
<pre class="r"><code>lbh1_reference &lt;- lbh_reference[lbh_reference$sound.files == &quot;lbh1.wav&quot;, ]

# diagnose
diagnose_detection(reference = lbh1_reference, detection = lbh1_reference)[, c(1:3,
    6:8)]</code></pre>
<pre><code>##   true.positives false.positives false.negatives overlap.to.true.positives
## 1             10               0               0                         1
##   recall precision
## 1      1         1</code></pre>
<p> </p>
<p>We will work mostly with a single sound file for convenience but the functions can work on several sound files at the time. The files should be found in a single working directory. Although the above example is a bit silly, it shows the basic diagnostic indices, which include basic detection theory indices (‘true.positives’, ‘false.positives’, ‘false.negatives’, ‘recall’ and ‘precision’) mentioned above. We can play around with the reference table to see how these indices can be used to spot imperfect detection routines (and hopefully improve them!). For instance, we can remove some signals to see how this is reflected in the diagnostics. Getting rid of some rows in ‘detection’, simulating a detection with some false negatives, will affect the recall but not the precision:</p>
<pre class="r"><code># create new table
lbh1_detection &lt;- lbh1_reference[3:9, ]

# print spectrogram
label_spectro(wave = lbh1, reference = lbh1_reference, detection = lbh1_detection,
    hop.size = 10, ovlp = 50, flim = c(1, 10))</code></pre>
<p><img src="ohun_files/figure-html/unnamed-chunk-6-1.png" width="960" /></p>
<pre class="r"><code># diagnose
diagnose_detection(reference = lbh1_reference, detection = lbh1_detection)[, c(1:3,
    6:8)]</code></pre>
<pre><code>##   true.positives false.positives false.negatives overlap.to.true.positives
## 1              7               0               3                         1
##   recall precision
## 1    0.7         1</code></pre>
<p> </p>
<p>Having some additional signals not in reference will do the opposite, reducing precision but not recall. We can do this simply by switching the tables:</p>
<pre class="r"><code># print spectrogram
label_spectro(wave = lbh1, detection = lbh1_reference, reference = lbh1_detection,
    hop.size = 10, ovlp = 50, flim = c(1, 10))</code></pre>
<p><img src="ohun_files/figure-html/unnamed-chunk-7-1.png" width="960" /></p>
<pre class="r"><code># diagnose
diagnose_detection(reference = lbh1_detection, detection = lbh1_reference)[, c(1:3,
    6:8)]</code></pre>
<pre><code>##   true.positives false.positives false.negatives overlap.to.true.positives
## 1              7               3               0                         1
##   recall precision
## 1      1       0.7</code></pre>
<p> </p>
<p>The function offers three additional diagnose metrics:</p>
<ul>
<li><strong>Split positives</strong>: target signals overlapped by more than 1 detecion</li>
<li><strong>Merged positives</strong>: number of cases in which 2 or more target signals in ‘reference’ were overlapped by the same detection</li>
<li><strong>Proportional overlap of true positives</strong>: ratio of the time overlap of true positives with its corresponding signal in the reference table</li>
</ul>
<p>In a perfect detection routine split and merged positives should be 0 while proportional overlap should be 1. We can shift the start of signals a bit to reflect a detection in which there is some mismatch to the reference table regarding to the time location of signals:</p>
<pre class="r"><code># create new table
lbh1_detection &lt;- lbh1_reference

# add &#39;noise&#39; to start
set.seed(18)
lbh1_detection$start &lt;- lbh1_detection$start + rnorm(nrow(lbh1_detection), mean = 0,
    sd = 0.1)

## print spectrogram
label_spectro(wave = lbh1, reference = lbh1_reference, detection = lbh1_detection,
    hop.size = 10, ovlp = 50, flim = c(1, 10))</code></pre>
<p><img src="ohun_files/figure-html/unnamed-chunk-8-1.png" width="960" /></p>
<pre class="r"><code># diagnose
diagnose_detection(reference = lbh1_reference, detection = lbh1_detection)</code></pre>
<pre><code>##   true.positives false.positives false.negatives split.positives
## 1             10               0               0               0
##   merged.positives overlap.to.true.positives recall precision f1.score
## 1                0                 0.5280701      1         1        1</code></pre>
<p> </p>
<p>In addition, the following diagnostics related to the duration of the signals can also be returned by setting <code>time.diagnostics = TRUE</code>. Here we tweak the reference and detection data just to have some false positives and false negatives:</p>
<pre class="r"><code># diagnose with time diagnostics
diagnose_detection(reference = lbh1_reference[-1, ], detection = lbh1_detection[-10,
    ], time.diagnostics = TRUE)</code></pre>
<pre><code>##   true.positives false.positives false.negatives split.positives
## 1              8               1               1               0
##   merged.positives overlap.to.true.positives mean.duration.true.positives
## 1                0                 0.6103203                    0.1387504
##   mean.duration.false.positives mean.duration.false.negatives
## 1                    0.05524073                       0.15845
##   proportional.duration.true.positives    recall precision  f1.score
## 1                                    1 0.8888889 0.8888889 0.8888889</code></pre>
<p> </p>
<p>These additional metrics can be used to further filter out undesired signals based on their duration (for instance in a energy-based detection as in <code>energy_detector()</code>, explained below).</p>
<p>Diagnostics can also be detailed by sound file:</p>
<pre class="r"><code># diagnose by sound file
diagnostic &lt;- diagnose_detection(reference = lbh1_reference, detection = lbh1_detection,
    by.sound.file = TRUE)

diagnostic</code></pre>
<pre><code>##   sound.files true.positives false.positives false.negatives split.positives
## 1    lbh1.wav             10               0               0               0
##   merged.positives overlap.to.true.positives recall precision f1.score
## 1                0                 0.5280701      1         1        1</code></pre>
<p> </p>
<p>These diagnostics can be summarized (as in the default <code>diagnose_detection()</code> output) with the function <code>summarize_diagnostic()</code>:</p>
<pre class="r"><code># summarize
summarize_diagnostic(diagnostic)</code></pre>
<pre><code>##   true.positives false.positives false.negatives split.positives
## 1             10               0               0               0
##   merged.positives overlap.to.true.positives recall precision f1.score
## 1                0                 0.5280701      1         1        1</code></pre>
<p> </p>
</div>
<div id="detecting-signals-with-ohun" class="section level1">
<h1>Detecting signals with <em>ohun</em></h1>
<div id="energy-based-detection" class="section level2">
<h2>Energy-based detection</h2>
<p>This detector uses amplitude envelopes to infer the position of signals. Amplitude envelopes are representations of the variation in energy through time. The following code plots an amplitude envelope along with the spectrogram for the example data <code>lbh1</code>:</p>
<pre class="r"><code># plot spectrogram and envelope
label_spectro(wave = cutw(lbh1, from = 0, to = 1.5, output = &quot;Wave&quot;), ovlp = 90,
    hop.size = 10, flim = c(0, 10), envelope = TRUE)</code></pre>
<p><img src="ohun_files/figure-html/unnamed-chunk-12-1.png" width="960" />  </p>
<p>This type of detector doesn’t require highly stereotyped signals, although they work better on high quality recordings in which the amplitude of target signals is higher than the background noise (i.e. high signal-to-noise ratio). The function <code>ernergy_detector()</code> performs this type of detection.</p>
<p> </p>
<div id="how-it-works" class="section level3">
<h3>How it works</h3>
<p>We can understand how to use <code>ernergy_detector()</code> using simulated signals. We will do that using the function <code>simulate_songs()</code> from <a href="https://CRAN.R-project.org/package=warbleR">warbleR</a>. In this example we simulate a recording with 10 sounds with two different frequency ranges and durations:</p>
<pre class="r"><code># install this package first if not installed install.packages(&#39;Sim.DiffProc&#39;)

# Creating vector for duration
durs &lt;- rep(c(0.3, 1), 5)

# Creating simulated song
set.seed(12)
simulated_1 &lt;- warbleR::simulate_songs(n = 10, durs = durs, freqs = 5, sig2 = 0.01,
    gaps = 0.5, harms = 1, bgn = 0.1, path = tempdir(), file.name = &quot;simulated_1&quot;,
    selec.table = TRUE, shape = &quot;cos&quot;, fin = 0.3, fout = 0.35, samp.rate = 18)$wave</code></pre>
<p> </p>
<p>The function call saves a ‘.wav’ sound file in a temporary directory (<code>tempdir()</code>) and also returns a <code>wave</code> object in the R environment. This outputs will be used to run energy-based detection and creating plots, respectively. This is how the spectrogram and amplitude envelope of the simulated recording look like:</p>
<pre class="r"><code># plot spectrogram and envelope
label_spectro(wave = simulated_1, env = TRUE, fastdisp = TRUE)</code></pre>
<p><img src="ohun_files/figure-html/unnamed-chunk-14-1.png" width="960" />  </p>
<p>Note that the amplitude envelope shows a high signal-to-noise ratio of the signals, which is ideal for energy-based detection. This can be conducted using <code>ernergy_detector()</code> as follows:</p>
<pre class="r"><code># run detection
detection &lt;- energy_detector(files = &quot;simulated_1.wav&quot;, bp = c(2, 8), threshold = 50,
    smooth = 150, path = tempdir())

# plot spectrogram and envelope
label_spectro(wave = simulated_1, envelope = TRUE, detection = detection, threshold = 50)</code></pre>
<p><img src="ohun_files/figure-html/unnamed-chunk-15-1.png" width="960" />  </p>
<p>The output is a selection table:</p>
<pre class="r"><code>detection</code></pre>
<pre><code>## Object of class &#39;selection_table&#39; 
## * The output of the following call: 
## energy_detector(files = &quot;simulated_1.wav&quot;, path = tempdir(),  
##  bp = c(2, 8), smooth = 150, threshold = 50) 
## 
## Contains: 
## *  A selection table data frame with 10 rows and 5 columns: 
## |sound.files     | duration| selec|  start|    end|
## |:---------------|--------:|-----:|------:|------:|
## |simulated_1.wav |   0.2328|     1| 0.5309| 0.7638|
## |simulated_1.wav |   0.7947|     2| 1.3955| 2.1901|
## |simulated_1.wav |   0.2334|     3| 2.8308| 3.0642|
## |simulated_1.wav |   0.7944|     4| 3.6955| 4.4899|
## |simulated_1.wav |   0.2333|     5| 5.1307| 5.3641|
## |simulated_1.wav |   0.7945|     6| 5.9956| 6.7901|
## ... and 4 more row(s) 
## 
##  * A data frame (check.results) generated by check_sels() (as attribute) 
## created by warbleR 1.1.27</code></pre>
<p>Now we will make use of some additional arguments to filter out specific signals based on their structural features. For instance we can use the argument <code>minimum.duration</code> to provide a time treshold (in ms) to exclude short signals and keep only the longest signals:</p>
<pre class="r"><code># run detection
detection &lt;- energy_detector(files = &quot;simulated_1.wav&quot;, bp = c(1, 8), threshold = 50,
    min.duration = 500, smooth = 150, path = tempdir())

# plot spectrogram
label_spectro(wave = simulated_1, detection = detection)</code></pre>
<p><img src="ohun_files/figure-html/unnamed-chunk-17-1.png" width="960" />  </p>
<p>We can use the argument <code>max.duration</code> (also in ms) to exclude long signals and keep the short ones:</p>
<pre class="r"><code># run detection
detection &lt;- energy_detector(files = &quot;simulated_1.wav&quot;, bp = c(1, 8), threshold = 50,
    smooth = 150, max.duration = 500, path = tempdir())

# plot spectrogram
label_spectro(wave = simulated_1, detection = detection)</code></pre>
<p><img src="ohun_files/figure-html/unnamed-chunk-18-1.png" width="960" />  </p>
<p>We can also focus the detection on specific frequency ranges using the argument <code>bp</code> (bandpass). By setting <code>bp = c(5, 8)</code> only those signals found within that frequency range (5-8 kHz) will be detected, which excludes signals below 5 kHz:</p>
<pre class="r"><code># Detecting
detection &lt;- energy_detector(files = &quot;simulated_1.wav&quot;, bp = c(5, 8), threshold = 50,
    smooth = 150, path = tempdir())

# plot spectrogram
label_spectro(wave = simulated_1, detection = detection)</code></pre>
<p><img src="ohun_files/figure-html/unnamed-chunk-19-1.png" width="960" />  </p>
<p>The same logic can be applied to detect those signals found below 5 kHz. We just need to set the upper bound of the band pass filter below the range of the higher frequency signals (for instance <code>bp = (0, 6)</code>):</p>
<pre class="r"><code># Detect
detection &lt;- energy_detector(files = &quot;simulated_1.wav&quot;, bp = c(0, 6), threshold = 50,
    min.duration = 1, smooth = 150, path = tempdir())

# plot spectrogram
label_spectro(wave = simulated_1, detection = detection)</code></pre>
<p><img src="ohun_files/figure-html/unnamed-chunk-20-1.png" width="960" />  </p>
<p>Amplitude modulation (variation in amplitude across a signal) can be problematic for detection based on amplitude envelopes. We can also simulate some amplitude modulation using <code>warbleR::simulate_songs()</code>:</p>
<pre class="r"><code># Creating simulated song
set.seed(12)

# Creating vector for duration
durs &lt;- rep(c(0.3, 1), 5)

sim_2 &lt;- sim_songs(n = 10, durs = durs, freqs = 5, sig2 = 0.01, gaps = 0.5, harms = 1,
    bgn = 0.1, path = tempdir(), file.name = &quot;simulated_2&quot;, selec.table = TRUE, shape = &quot;cos&quot;,
    fin = 0.3, fout = 0.35, samp.rate = 18, am.amps = c(1, 2, 3, 2, 0.1, 2, 3, 3,
        2, 1))

# extract wave object and selection table
simulated_2 &lt;- sim_2$wave
sim2_sel_table &lt;- sim_2$selec.table

# plot spectrogram
label_spectro(wave = simulated_2, envelope = TRUE)</code></pre>
<p><img src="ohun_files/figure-html/unnamed-chunk-21-1.png" width="960" />  </p>
<p>When signals have strong amplitude modulation they can be split during detection:</p>
<pre class="r"><code># detect sounds
detection &lt;- energy_detector(files = &quot;simulated_2.wav&quot;, threshold = 50, path = tempdir())

# plot spectrogram
label_spectro(wave = simulated_2, envelope = TRUE, threshold = 50, detection = detection)</code></pre>
<p><img src="ohun_files/figure-html/unnamed-chunk-22-1.png" width="960" />  </p>
<p>There are two arguments that can deal with this: <code>holdtime</code> and <code>smooth</code>. <code>hold.time</code> allows to merge split signals that are found within a given time range (in ms). This time range should be high enough to merge things belonging to the same signal but not too high so it merges different signals. For this example a <code>hold.time</code> of 200 ms can do the trick (we know gaps between signals are ~0.5 s long):</p>
<pre class="r"><code># detect sounds
detection &lt;- energy_detector(files = &quot;simulated_2.wav&quot;, threshold = 50, min.duration = 1,
    path = tempdir(), hold.time = 200)

# plot spectrogram
label_spectro(wave = simulated_2, envelope = TRUE, threshold = 50, detection = detection)</code></pre>
<p><img src="ohun_files/figure-html/unnamed-chunk-23-1.png" width="960" />  </p>
<p><code>smooth</code> works by merging the amplitude envelope ‘hills’ of the split signals themselves. It smooths envelopes by applying a sliding window averaging of amplitude values. It’s given in ms of the window size. A <code>smooth</code> of 350 ms can merged back split signals from our example:</p>
<pre class="r"><code># detect sounds
detection &lt;- energy_detector(files = &quot;simulated_2.wav&quot;, threshold = 50, min.duration = 1,
    path = tempdir(), smooth = 350)

# plot spectrogram
label_spectro(wave = simulated_2, envelope = TRUE, threshold = 50, detection = detection,
    smooth = 350)</code></pre>
<p><img src="ohun_files/figure-html/unnamed-chunk-24-1.png" width="960" />  </p>
<p>The function has some additional arguments for further filtering detections (<code>peak.amplitude</code>) and speeding up analysis (<code>thinning</code> and <code>parallel</code>).</p>
<p> </p>
</div>
<div id="optimizing-energy-based-detection" class="section level3">
<h3>Optimizing energy-based detection</h3>
<p>This last example using <code>smooth</code> can be used to showcase how the tunning parameters can be optimized. As explained above, to do this we need a reference table that contains the time position of the target signals. The function <code>optimize_energy_detector()</code> can be used finding the optimal parameter values. We must provide the range of parameter values that will be evaluated:</p>
<pre class="r"><code>optim_detection &lt;- optimize_energy_detector(reference = sim2_sel_table, files = &quot;simulated_2.wav&quot;,
    threshold = 50, min.duration = 1, path = tempdir(), smooth = c(100, 250, 350))</code></pre>
<pre><code>## 3 combinations will be evaluated:</code></pre>
<pre class="r"><code>optim_detection[, c(1, 2:5, 7:12, 17:18)]</code></pre>
<pre><code>##   threshold peak.amplitude smooth hold.time min.duration thinning
## 1        50              0    100         0            1        1
## 2        50              0    250         0            1        1
## 3        50              0    350         0            1        1
##   true.positives false.positives false.negatives split.positives
## 1             10               0               0              10
## 2             10               0               0               5
## 3             10               0               0               0
##   merged.positives proportional.duration.true.positives duty.cycle
## 1                0                             1.000000  0.3714601
## 2                0                             1.179487  0.3555202
## 3                0                             1.000000  0.4442706</code></pre>
<p> </p>
<p>The output contains the combination of parameters used at each iteration as well as the corresponding diagnose indices. In this case all combinations generate a good detection (recall &amp; precision = 1). However, only the routine with the highest <code>smooth</code> (last row) has no split signals (‘split.positive’ column). It also shows a better overlap to the reference signals (‘overlap.to.true.positives’ closer to 1).</p>
<p>In addition, there are two complementary functions for optimizing energy-based detection routines: <code>feature_reference()</code> and <code>merge_overlaps()</code>. <code>feature_reference()</code> allow user to get a sense of the time and frequency characteristics of a reference table. This information can be used to determine the range of tuning parameter values during optimization. This is the output of the function applied to <code>lbh_reference</code>:</p>
<pre class="r"><code>feature_reference(reference = lbh_reference, path = tempdir())</code></pre>
<pre><code>##                   min   mean    max
## sel.duration   117.96 142.60 163.73
## gap.duration   624.97 680.92 811.61
## duty.cycle       0.24   0.27   0.31
## peak.amplitude  47.74  55.56  62.01
## bottom.freq      1.81   2.11   2.37
## top.freq         8.49   8.82   9.53</code></pre>
<p> </p>
<p>Features related to selection duration can be used to set the ‘max.duration’ and ‘min.duration’ values, frequency related features can inform banpass values, gap related features inform hold time values and duty cycle can be used to evaluate performance. Peak amplitude can be used to keep only those signals with the highest intensity, mostly useful for routines in which only a subset of the target signals present in the recordings is needed.</p>
<p><code>merge_overlaps()</code> finds time-overlapping selections in reference tables and collapses them into a single selection. Overlapping selections would more likely appear as a single amplitude ‘hill’ and thus would be detected as a single signal. So <code>merge_overlaps()</code> can be useful to prepare references in a format representing a more realistic expectation of how a pefect energy detection routine would look like.</p>
</div>
</div>
<div id="template-based-detection" class="section level2">
<h2>Template-based detection</h2>
<p>This detection method is better suited for highly stereotyped signals. As it doesn’t depend on the signal-to-noise ratio it’s more robust to higher levels of background noise. The procedure is divided in three steps:</p>
<ul>
<li>Choosing the right template (<code>get_templates()</code>)</li>
<li>Estimating the cross-correlation scores of templates along sound files (<code>template_correlator()</code>)<br />
</li>
<li>Detecting signals by applying a correlation threshold (<code>template_detector()</code>)</li>
</ul>
<p>The function <code>get_templates()</code> can help you find a template closer to the average acoustic structure of the signals in a reference table. This is done by finding the signals closer to the centroid of the acoustic space. When the acoustic space is not supplied (‘acoustic.space’ argument) the function estimates it by measuring several acoustic parameters using the function <a href="https://marce10.github.io/warbleR/reference/spectro_analysis.html"><code>spectro_analysis()</code></a> from <a href="https://marce10.github.io/warbleR"><code>warbleR</code></a>) and summarizing it with Principal Component Analysis (after z-transforming parameters). If only 1 template is required the function returns the signal closest to the acoustic space centroid. The rationale here is that a signal closest to the average signal structure is more likely to share structural features with most signals across the acoustic space than a signal in the periphery of the space. These ‘mean structure’ templates can be obtained as follows:</p>
<pre class="r"><code># get mean structure template
template &lt;- get_templates(reference = lbh1_reference, path = tempdir())</code></pre>
<pre><code>## 3 principal components were kept which explained 0.73 of the variance</code></pre>
<p> </p>
<p>Then, the template can be used to detect similar signals in the example ‘lbh1’ data:</p>
<pre class="r"><code># get correlations
correlations &lt;- template_correlator(templates = template, files = &quot;lbh1.wav&quot;, path = tempdir())</code></pre>
<p> </p>
<p>The output is an object of class ‘template_correlations’, with its own printing method:</p>
<pre class="r"><code># print
correlations</code></pre>
<pre><code>## Object of class &#39;template_correlations&#39; 
## * The output of the following template_correlator() call: 
## template_correlator(templates = template, files = &quot;lbh1.wav&quot;,  
##  path = tempdir()) 
## * Contains 1 correlation score vector(s) from 1 template(s):
##  lbh1.wav-16 
## ... and 1 sound files(s):
##  lbh1.wav 
##  * Created by ohun 0.1.0</code></pre>
<p> </p>
<p>This object can then be used to detect signals using <code>template_detector()</code>:</p>
<pre class="r"><code># run detection
detection &lt;- template_detector(template.correlations = correlations, threshold = 0.4)

detection</code></pre>
<pre><code>## Object of class &#39;selection_table&#39; 
## * The output of the following call: 
## template_detector(template.correlations = correlations, threshold = 0.4) 
## 
## Contains: 
## *  A selection table data frame with 16 rows and 6 columns: 
## |sound.files | selec|  start|    end|template    | scores|
## |:-----------|-----:|------:|------:|:-----------|------:|
## |lbh1.wav    |     1| 0.0816| 0.2347|lbh1.wav-16 | 0.7158|
## |lbh1.wav    |     2| 0.5709| 0.7241|lbh1.wav-16 | 0.7246|
## |lbh1.wav    |     3| 1.0602| 1.2134|lbh1.wav-16 | 0.6442|
## |lbh1.wav    |     4| 1.1302| 1.2833|lbh1.wav-16 | 0.4265|
## |lbh1.wav    |     5| 1.3399| 1.4930|lbh1.wav-16 | 0.4010|
## |lbh1.wav    |     6| 1.7127| 1.8659|lbh1.wav-16 | 0.7724|
## ... and 10 more row(s) 
## 
##  * A data frame (check.results) generated by check_sels() (as attribute) 
## created by warbleR 1.1.27</code></pre>
<p> </p>
<p>The output can be explored by plotting the spectrogram along with the detection and correlation scores:</p>
<pre class="r"><code># plot spectrogram
label_spectro(wave = lbh1, detection = detection, template.correlation = correlations$`lbh1.wav-10/lbh1.wav`,
    flim = c(0, 10), threshold = 0.4, hop.size = 10, ovlp = 50)</code></pre>
<p><img src="ohun_files/figure-html/unnamed-chunk-31-1.png" width="960" />  </p>
<p>The performance can be evaluated using <code>diagnose_detection()</code>:</p>
<pre class="r"><code># diagnose
diagnose_detection(reference = lbh1_reference, detection = detection)</code></pre>
<pre><code>##   true.positives false.positives false.negatives split.positives
## 1             10               1               0               5
##   merged.positives overlap.to.true.positives recall precision f1.score
## 1                0                 0.8610092      1 0.9090909 0.952381</code></pre>
<p> </p>
<div id="optimizing-template-based-detection" class="section level3">
<h3>Optimizing template-based detection</h3>
<p>The function <code>optimize_template_detector()</code> allows to evaluate the performance under different correlation thresholds:</p>
<pre class="r"><code># run optimization
optimization &lt;- optimize_template_detector(template.correlations = correlations,
    reference = lbh1_reference, threshold = seq(0.1, 0.5, 0.1))</code></pre>
<pre><code>## 5 thresholds will be evaluated:</code></pre>
<pre class="r"><code># print output
optimization</code></pre>
<pre><code>##   threshold   templates true.positives false.positives false.negatives
## 1       0.1 lbh1.wav-16             10              43               0
## 2       0.2 lbh1.wav-16             10              32               0
## 3       0.3 lbh1.wav-16             10               3               0
## 4       0.4 lbh1.wav-16             10               1               0
## 5       0.5 lbh1.wav-16             10               0               0
##   split.positives merged.positives overlap.to.true.positives recall precision
## 1              10                0                 0.5100857      1 0.1886792
## 2              10                0                 0.5826374      1 0.2380952
## 3              10                0                 0.6703757      1 0.7692308
## 4               5                0                 0.8610092      1 0.9090909
## 5               0                0                 0.9611086      1 1.0000000
##    f1.score
## 1 0.3174603
## 2 0.3846154
## 3 0.8695652
## 4 0.9523810
## 5 1.0000000</code></pre>
<p> </p>
<p>Additional threshold values can be evaluated without having to run it all over again. We just need to supplied the output from the previous run with the argument <code>previous.output</code> (the same trick can be done when optimizing an energy-based detection):</p>
<pre class="r"><code># run optimization
optimize_template_detector(template.correlations = correlations, reference = lbh1_reference,
    threshold = c(0.6, 0.7), previous.output = optimization)</code></pre>
<pre><code>## 2 thresholds will be evaluated:</code></pre>
<pre><code>##   threshold   templates true.positives false.positives false.negatives
## 1       0.1 lbh1.wav-16             10              43               0
## 2       0.2 lbh1.wav-16             10              32               0
## 3       0.3 lbh1.wav-16             10               3               0
## 4       0.4 lbh1.wav-16             10               1               0
## 5       0.5 lbh1.wav-16             10               0               0
## 6       0.6 lbh1.wav-16             10               0               0
## 7       0.7 lbh1.wav-16              7               0               3
##   split.positives merged.positives overlap.to.true.positives recall precision
## 1              10                0                 0.5100857    1.0 0.1886792
## 2              10                0                 0.5826374    1.0 0.2380952
## 3              10                0                 0.6703757    1.0 0.7692308
## 4               5                0                 0.8610092    1.0 0.9090909
## 5               0                0                 0.9611086    1.0 1.0000000
## 6               0                0                 0.9611086    1.0 1.0000000
## 7               0                0                 0.9545278    0.7 1.0000000
##    f1.score
## 1 0.3174603
## 2 0.3846154
## 3 0.8695652
## 4 0.9523810
## 5 1.0000000
## 6 1.0000000
## 7 0.8235294</code></pre>
<p> </p>
<p>In this case several threshold values can achieved an optimal detection.</p>
<p> </p>
</div>
<div id="detecting-several-templates" class="section level3">
<h3>Detecting several templates</h3>
<p>Several templates can be used within the same call. Here we correlate two templates on the two example sound files, taking one template from each sound file:</p>
<pre class="r"><code># get correlations
correlations &lt;- template_correlator(templates = lbh_reference[c(1, 10), ], files = c(&quot;lbh1.wav&quot;,
    &quot;lbh2.wav&quot;), path = tempdir())

# run detection
detection &lt;- template_detector(template.correlations = correlations, threshold = 0.5)</code></pre>
<p> </p>
<p>Note that in these cases we can get the same signal detected several times (duplicates), one by each template. We can check if that is the case just by diagnosing the detection:</p>
<pre class="r"><code># diagnose
diagnose_detection(reference = lbh_reference, detection = detection)</code></pre>
<pre><code>##   true.positives false.positives false.negatives split.positives
## 1             19               0               0               4
##   merged.positives overlap.to.true.positives recall precision f1.score
## 1                0                 0.9565014      1         1        1</code></pre>
<p> </p>
<p>Duplicates are shown as split positives. Fortunately, we can leave a single detected signal by leaving only those with the highest correlation. To do this we first need to label each row in the detection using <code>label_detection()</code> and then remove duplicates using <code>filter_detection()</code>:</p>
<pre class="r"><code># labeling detection
labeled &lt;- label_detection(reference = lbh_reference, detection = detection)</code></pre>
<p>This function adds a column (‘detection.class’) with the class label for each row:</p>
<pre class="r"><code>table(labeled$detection.class)</code></pre>
<pre><code>## 
##         true.positive true.positive (split) 
##                    15                     8</code></pre>
<p> </p>
<p>Now we can filter out duplicates and diagnose the detection again, telling the function to select a single row per duplicate using the correlation score as a criterium (<code>by = "scores"</code>, this column is part of the <code>template_detector()</code> output):</p>
<pre class="r"><code># filter
filtered &lt;- filter_detection(detection = labeled, by = &quot;scores&quot;)

# diagnose
diagnose_detection(reference = lbh_reference, detection = filtered)</code></pre>
<pre><code>##   true.positives false.positives false.negatives split.positives
## 1             19               0               0               0
##   merged.positives overlap.to.true.positives recall precision f1.score
## 1                0                 0.9501523      1         1        1</code></pre>
<p> </p>
<p>We successfully get rid of duplicates and detected every single target signal.</p>
<hr />
</div>
</div>
<div id="improving-detection-speed" class="section level2">
<h2>Improving detection speed</h2>
<p>Detection routines can take a long time when working with large amounts of acoustic data (e.g. large sound files and/or many sound files). These are some useful points to keep in mine when trying to make a routine more time-efficient:</p>
<ul>
<li>Always test procedures on small data subsets</li>
<li><code>template_detector()</code> is faster than <code>energy_detector()</code></li>
<li>Parallelization (see <code>parallel</code> argument in most functions) can significantly speed-up routines, but works better on Unix-based operating systems (linux and mac OS)</li>
<li>Sampling rate matters: detecting signals on low sampling rate files goes faster, so we should avoid having nyquist frequencies (sampling rate / 2) way higher than the highest frequency of the target signals (sound files can be downsampled using warbleR’s <a href="https://marce10.github.io/warbleR/reference/selection_table.html"><code>fix_sound_files()</code></a>)</li>
<li>Large sound files can make the routine crash, use <code>split_acoustic_data()</code> to split both reference tables and files into shorter clips.</li>
<li>Think about using a computer with lots of RAM memory or a computer cluster for working on large amounts of data</li>
<li><code>thinning</code> argument (which reduces the size of the amplitude envelope) can also speed-up <code>energy_detector()</code></li>
</ul>
<p> </p>
<hr />
</div>
<div id="additional-tips" class="section level2">
<h2>Additional tips</h2>
<ul>
<li>Use your knowledge about the signal structure to determine the initial range for the tuning parameters in a detection optimization routine</li>
<li>If people have a hard time figuring out where a target signal occurs in a recording, detection algorithms will also have a hard time</li>
<li>Several templates representing the range of variation in signal structure can be used to detect semi-stereotyped signals</li>
<li>Make sure reference tables contain all target signals and only the target signals. The performance of the detection cannot be better than the reference itself.</li>
<li>Avoid having overlapping signals or several signals as a single one (like a multi-syllable vocalization) in the reference table when running an energy-based detector</li>
<li>Low-precision can be improved by training a classification model (e.g. random forest) to tell signals from noise</li>
</ul>
<hr />
<div class="alert alert-info">
<p>Please cite <a href="https://github.com/maRce10/ohun">ohun</a> like this:</p>
<p>Araya-Salas, M. (2021), <em>ohun: automatic detection of acoustic signals</em>. R package version 0.1.0.</p>
</div>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<ol style="list-style-type: decimal">
<li>Araya-Salas, M. (2021), ohun: automatic detection of acoustic signals. R package version 0.1.0.</li>
<li>Araya-Salas M, Smith-Vidaurre G (2017) warbleR: An R package to streamline analysis of animal acoustic signals. Methods Ecol Evol 8:184-191.</li>
<li>Khanna H., Gaunt S.L.L. &amp; McCallum D.A. (1997). Digital spectrographic cross-correlation: tests of sensitivity. Bioacoustics 7(3): 209-234.</li>
<li>Macmillan, N. A., &amp; Creelman, C.D. (2004). Detection theory: A user’s guide. Psychology press.</li>
</ol>
<p> </p>
<hr />
<p><font size="4">Session information</font></p>
<pre><code>## R version 4.1.0 (2021-05-18)
## Platform: x86_64-pc-linux-gnu (64-bit)
## Running under: Ubuntu 20.04.2 LTS
## 
## Matrix products: default
## BLAS:   /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3.10.3
## LAPACK: /usr/lib/x86_64-linux-gnu/atlas/liblapack.so.3.10.3
## 
## locale:
##  [1] LC_CTYPE=pt_BR.UTF-8       LC_NUMERIC=C              
##  [3] LC_TIME=es_CR.UTF-8        LC_COLLATE=pt_BR.UTF-8    
##  [5] LC_MONETARY=es_CR.UTF-8    LC_MESSAGES=pt_BR.UTF-8   
##  [7] LC_PAPER=es_CR.UTF-8       LC_NAME=C                 
##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
## [11] LC_MEASUREMENT=es_CR.UTF-8 LC_IDENTIFICATION=C       
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] ohun_0.1.0         warbleR_1.1.27     NatureSounds_1.0.4 knitr_1.39        
## [5] seewave_2.2.0      tuneR_1.4.0       
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.1.1  xfun_0.31         bslib_0.2.5.1     purrr_0.3.4      
##  [5] pbapply_1.5-0     colorspace_2.0-2  vctrs_0.3.8       generics_0.1.0   
##  [9] viridisLite_0.4.0 htmltools_0.5.2   yaml_2.3.5        utf8_1.2.2       
## [13] rlang_1.0.2       jquerylib_0.1.4   pillar_1.6.4      DBI_1.1.1        
## [17] glue_1.6.2        lifecycle_1.0.1   stringr_1.4.0     munsell_0.5.0    
## [21] gtable_0.3.0      evaluate_0.15     fastmap_1.1.0     fftw_1.0-7       
## [25] parallel_4.1.0    fansi_1.0.0       highr_0.9         Rcpp_1.0.8.3     
## [29] formatR_1.11      scales_1.1.1      jsonlite_1.8.0    soundgen_2.1.0   
## [33] Sim.DiffProc_4.8  Deriv_4.1.3       gridExtra_2.3     rjson_0.2.21     
## [37] ggplot2_3.3.5     digest_0.6.29     stringi_1.7.6     dplyr_1.0.7      
## [41] dtw_1.22-3        grid_4.1.0        cli_3.1.0         tools_4.1.0      
## [45] bitops_1.0-7      magrittr_2.0.3    sass_0.4.0        RCurl_1.98-1.6   
## [49] proxy_0.4-26      tibble_3.1.6      crayon_1.5.1      pkgconfig_2.0.3  
## [53] MASS_7.3-54       ellipsis_0.3.2    shinyBS_0.61      assertthat_0.2.1 
## [57] rmarkdown_2.13    rstudioapi_0.13   viridis_0.6.2     R6_2.5.1         
## [61] signal_0.7-7      compiler_4.1.0</code></pre>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = false;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
